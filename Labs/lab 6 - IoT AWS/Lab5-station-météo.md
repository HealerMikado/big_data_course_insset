# Cr√©ation d'un pipeline d'analyse de donn√©e pour IoT

## üéØObjectifs 

Ce TP a pour but de mettre en place un pipeline complet d'analyse de donn√©e IoT.  Les donn√©es utilis√©es proviennent des donn√©es de stations m√©t√©os mise √† disposition par la NOAA ([National Oceanic and Atmospheric Administration](https://www.noaa.gov/)). Comme il n'est pas possible de se connecter directement aux stations, vous allez cr√©er des stations fictives qui produiront des donn√©es d√©j√† mesur√©es, √† un rythme beaucoup plus rapide que normalement pour avoir rapidement beaucoup de donn√©es.

Tout ce pipeline sera d√©ploy√© sur AWS et mobilisera les services suivants :

- AWS IoT Core pour enregistrer les stations m√©t√©os et r√©cup√©rer les messages ;
- Amazon Kinesis Data Firehose pour charger les messages en quasi temps r√©el ;
- Amazon S3 pour stoker les donn√©es ;
- AWS Glue pour d√©couvrir le sch√©ma des donn√©es ;
- Amazon Athena pour requ√™ter les donn√©es via SQL ;
- Amazon Quicksight pour visualiser les donn√©es.

> Tout ce que nous allons mettre en place peut-√™tre mis en place autrement. Mais AWS propose des outils clefs en main ce qui nous permet de pouvoir faire tout cela en peu de temps. Si le service IoT de Google va disparaitre en aout 2023, Microsoft Azure propose une suite IoT et analytics qui pourraient remplacer celles d'AWS dans ce TP. De plus, des solutions open source existent √©galement. Elles seraient par contre trop longue √† mettre en place pour un seul TP.

Voici le sch√©ma d'architecture final du TP :

![](assets/target architecture.jpg)

## üß±Mise en place

1. R√©cup√©rez les code du TP sur Git ou Moodle
2. Connectez-vous √† AWS Academy et cherchez le service S3. Cr√©ez les buckets suivants (ajoutez √† chaque fois une suite de caract√®res pour rendre le nom du bucket unique):
   - iot-tutorial-things-template : c'est ici que le script d'initialisation uploadera les fichiers pour g√©n√©rer les stations m√©t√©os.
   - iot-tutorial-datalake : Amazon Kinesis Data Firehose dumpera les donn√©es ici
   - iot-tutorial-athena-query : c'est dans ce bucket qu'Amazon Athena √©crira les r√©sultats de ses requ√™tes 
3. Cherchez le service AWS IoT Core et dans le menu √† gauche cherchez `settings`. Copiez la valeur de votre endpoint dans le fichier .`env` qui se trouve dans le dossier `docker` du TP
4. Cliquez sur votre nom d'utilisateur en haut √† droite de la console AWS puis copiez/collez votre account ID dans un bloc-note
5. Installez toutes les d√©pendances python du TP qui se trouve dans le fichier `requirements.txt` √† la racine du TP

## üì±Mon premier objet

Vous allez maintenant cr√©er un objet dans la console d'AWS et faire √† la mains les √©tapes qui seront automatis√©es par la suite. Cherchez dans le menu `Things` et une fois dessus cliquez sur `Create thing` puis `Create single thing`. Donnez le nom que vous souhaitez √† votre objet. Ajoutez si vous le souhaitez un type √† votre objet (vous devez alors en cr√©er un) et des attributs. N'ajoutez pas de `shadow` √† votre objet. Passez √† l'√©tape d'apr√®s. Laissez AWS g√©n√©rer un certificat pour votre objet.

Maintenant nous allons cr√©er une policy qui va d√©terminer ce que notre objet peut faire. AWS fonctionne avec un principe implicite deny, donc si on ne dit rien, notre objet ne pourra rien faire (pas m√™me envoyer un message). Cliquez sur `Create a policy`, donnez le nom que vous souhaitez, et pour l'action choisissiez `*` et pour la ressource cible `*` √©galement. Valider la policy

>Il ne faut absolument pas faire √ßa en production ! On vient de cr√©er une policy qui donne tout les droits sur les actions du service IoT. Il faudrait limiter les droits aux actions que nous allons faire, ici se connecter et publier. Mais dans un souci de simplicit√© cette policy convient tr√®s bien pour ce TP.

Retournez sur l'√©cran de la cr√©ation de l'objet et cr√©ez-le. T√©l√©chargez le certificat de votre objet, les deux clefs, et le certificat Root. Mettez-les dans le  dossier `single-thing` du dossier `single-thing`.

### Connection MQTT avec les outils AWS

Pour r√©aliser la connexion MQTT entre notre endpoint AWS et notre objet, nous allons utiliser les packages :

- [awscrt](https://pypi.org/project/awscrt/) : AWS Common Runtime pour cr√©er un thread pour ex√©cuter des t√¢ches asynchrone
- [awsiot](https://pypi.org/project/awsiotsdk/) : le sdk AWS IoT pour √©tablir la connexion MQTT

Voici le code de base

```python
from awscrt import io, mqtt
from awsiot import mqtt_connection_builder

# Create a thread for async work
event_loop_group = io.EventLoopGroup(1)
# We use the basic DNS resolver
host_resolver = io.DefaultHostResolver(event_loop_group)
# Scoket creation
client_bootstrap = io.ClientBootstrap(event_loop_group, host_resolver)
# MQTT connection 
mqtt_connection = mqtt_connection_builder.mtls_from_path(
            endpoint=ENDPOINT,
            cert_filepath=PATH_TO_CERTIFICATE,
            pri_key_filepath=PATH_TO_PRIVATE_KEY,
            client_bootstrap=client_bootstrap,
            ca_filepath=PATH_TO_AMAZON_ROOT_CA_1,
            client_id=CLIENT_ID,
            clean_session=False,
            keep_alive_secs=6
            )
print("Connecting to {} with client ID '{}'...".format(
        ENDPOINT, CLIENT_ID))
# Make the connect() call
connect_future = mqtt_connection.connect()
# Future.result() waits until a result is available
connect_future.result()
print("Connected!")
```

Une fois la connection √©tablie, vous pouvez envoyer simplement un message avec la m√©thode [publish](https://awslabs.github.io/aws-crt-python/api/mqtt.html#awscrt.mqtt.Connection.publish)

```python
mqtt_connection.publish(topic=TOPIC, payload=json.dumps(data), qos=mqtt.QoS.AT_LEAST_ONCE)
```

#### ‚úç Hands-on 

Programmez une boucle pour envoyer des messages avec la m√©thode publish. Maintenant allez sur `MQTT test client`, abonnez vous au topic `hello/world` et lancer le fichier `send_message.py` en vous pla√ßant bien dans le dossier `single_thing`. L'√©cran devrait se remplir avec vos message.

## üöÑCr√©ation du pipeline

### ‚õÖCr√©ation de la flotte de stations m√©t√©o

Pour rendre le TP plus r√©aliste, vous allez cr√©ez 5 stations m√©t√©os fictives. Mais pour AWS elles seront de vrais objets connect√©s. Pour automatiser ce processus allez dans le dossier `registering-things-in-bulk`, et trouvez le fichier `config.py`. Modifiez la valeur des variables suivantes :

- ROLE_ARN en rempla√ßant `ACCOUNT_ID` par l'id de votre compte
- BUCKET_NAME en mettant le bucket avec le pr√©fixe `iot-tutorial-things-template`

> Ce script de cr√©ation d'objet en masse n'est pas de moi et provient du d√©p√¥t github suivant https://github.com/yilmaznaslan/aws-iot-core-registering-things-in-bulk. Merci √† l'auteur pour le temps gagner √† ne pas refaire ce script !

Cherchez √©galement le fichier `.env` qui se trouve dans le dosssier `registering-things-in-bulk/docker/` et modifiez la valeur de `ENDPOINT` avec la valeur de votre endpoint. Laissez tout le reste tel quel.

Placer-vous dans le dossier `scripts` et ex√©cutez le fichier `create_things.py`. Regardez dans la pages `Things` de la console AWS que 5 nouveaux objets sont apparus. Maintenant ex√©cutez le fichier `run_things`. Il va cr√©er 5 conteneurs Docker qui vont publier des donn√©es m√©t√©os en se basant sur des donn√©es m√©t√©orologiques pass√©es. Abonnez-vous au topic `sensor/weather`pour regarder si des donn√©es sont bien post√©es. Si c'est le cas vous √™tes bon pour passer √† l'√©tape suivante.

### üì¶Mon premier data warehouse : Amazon S3

Dans cette partie nous allez mettre en place la premi√®re brique de notre pipeline en stockant vos donn√©es. En effet pour le moment les donn√©es envoy√©es ne vont nul part et sont juste perdu. Nous allons utiliser S3 (Simple Storage Service) comme entrep√¥t de donn√©es. S3 est un service de stockage objet, peu cher, sans limite de volume. C'est la meilleure fa√ßon de stocker simplement des donn√©es (tous les services cloud proposent un services similaire). Par contre il faudra des outils suppl√©mentaire pour requ√™ter nos donn√©es.

Pour diriger nos donn√©es vers le service S3 nous allons devoir mettre en place une r√®gles pour router nos donn√©es. Cliquez sur `Rule` dans la rubrique `Act` puis `Create a rule`. Donnez le nom que vous voulez √† votre r√®gle (par exemple `weather_sensor_to_S3`, et une description si vous le souhaitez.

La page suivante permet de d√©terminer sur quelles donn√©es la r√®gles va √™tre appliqu√©e. Cela se fait avec une requ√™te SQL de la forme

```sql
SELECT field1, field2, field3 FROM "topic" WHERE condition
```

Cela permet de routage fin des messages. Par exemple la requ√™te

```SQL
SELECT * FROM FROM "sensor/weather" WHERE wind_speed > 20
```

Permet de router l'int√©gralit√© des variables des messages du topic `sensor/weather` si la wind_speed est sup√©rieure √† 20. Les messages avec une wind_speed inf√©rieure √† 20 ne sont pas affect√©s par cette r√®gle.

‚úç Hands-on : Nous voulons conserver toutes les donn√©es de tous les messages. Faite la r√®gles pour respecter cette condition. 

La page suivante permet de d√©finir o√π sont rout√©es les donn√©es. Comme nous nous souhaitons faire un data warehouse en utilisant S3 choisissez l'action `S3 bucket`. Choisissiez le d√©p√¥t S3 que vous avez fait en d√©but de TP,  pour le champ key saisissez`${topic()}/${timestamp()` (cela va cr√©er un dossier avec le nom du topic et chaque fichier sera nomm√© d'apr√®s son timestamp de cr√©ation), et laissez le champ `canned ACL` √† private. Enfin pour le IAM role choisissez `LabRole`. Enfin validez votre pipeline, et allez voire dans votre bucket S3.

Vous trouverez arborescence suivante `sensor/weather` avec une myriade de fichiers. 1 par message exactement. Ce comportement n'est pas celui que nous cherchons, car la multiplication des fichiers va entrainer une augmentation des co√ªts car acc√©der √† un fichier co√ªte un peu. La solution est nous allons mettre un buffer entre nos messages et notre data warehouse.

### üöøCharger les messages efficacement : Amazon Kinesis Data Firehose

Le buffer que nous allons utiliser est Amazon Kinesis Data Firehose. KDF est un service serverless d'ETL (Extract, Transform and Load) qui permet d'ing√©rer et livrer des donn√©es dans un data warehouse en quasi temps r√©el (d√©calage de l'ordre de la minute). KDF est enti√®rement manag√© par AWS, comme IoT core, ce qu'il fait qu'il n'y a aucune infrastructure √† g√©rer et que le service s'adapte automatiquement √† nos besoins.

> Comme vous allez le constater l'int√©gralit√© des services que nous utilisons dans le TP son serverless. Ces services ont comme avantage premier de permettre de d√©velopper rapidement une solution car il ne faut pas r√©fl√©chir √† la taille de l'infrastructure n√©cessaire. Par contre, des serveurs sont bien mobilis√©s chez AWS, nous n'y avons juste pas acc√®s.

Modifiez la r√®gle que vous venez de cr√©er et supprimez son action pour en cr√©er une `Kinesis Firehose stream`, puis cr√©ez un stream Firehose. Pour la source s√©lectionnez `Direct PUT`, et `S3` comme destination. Donnez un nom √† ce stream, comme `IoT_warehouse_delievery_stream` et s√©lectionnez votre bucket S3. Avant de valider, d√©pliez l'onglet `Advanced Settings`, et dans la partie permissions s√©lectionnez `Choose existing IAM role ` et `LabRole`. Validez votre stream.

Retournez sur la page de la d√©finition de votre action et s√©lectionnez le stream que vous venez de cr√©er. Conserver l'option `No separator`. Les messages seront mis bout √† bous sur des lignes diff√©rentes. Si vous s√©lectionnez le s√©parateur `\n` vous aurez des lignes vides dans vos fichiers. Enfin s√©lectionner le r√¥le `LabRole`.  Validez tout et attendez quelques minutes. Retourner dans votre bucket et vous devrez voir des dossiers apparaitre. Descendez dans arborescence et t√©l√©chargez un des fichiers. Le fichier contiendra plusieurs enregistrement cette fois-ci.

Voil√† vous venez de mettre en place la premi√®re partie de notre pipeline de traitement √† savoir l'acquisition et la persistance des fichiers.

#### ‚úç Hands-on : utilisation d'Amazon DynamoDB

Amazon DynamoDB est un service serverless offrant une base de donn√©e NoSQL. Vous allez stocker certaines donn√©es une table DynamoDB.

- Cr√©ez une nouvelle r√®gle que vous appellerez `weather_sensor_dynamoDB`

- Pour le filtre SQL, vous allez :

  - Ne conserver que les attributs suivants :

    -  weather_station, 
    - latitude, 
    - longitude, 
    - elevation, time, 
    - air_temperature.value
    - air_temperature.quality
    - wind_speed.value
    - weather_condition, 
    - sky_cover_condition 

    Pour les attributs imbriqu√©s, vous allez devoir les renommer avec un `as` : `parent.child as parent_child`

  - Utiliser le topic pr√©c√©dent

  - Ne prendre que les donn√©es o√π la temp√©rature est inf√©rieure √† -5

- L'action sera une action DynamoDBv2. Comme vous n'avez pas encore de table DynamoDB vous allez en cr√©er une. Donnez lui le nom que vous souhaitez, la partition key sera `weather_station` et la sort key `time`. √Ä elles deux elles formes la clef primaire de la table. Validez la cr√©ation de votre table, puis retournez dans l'√©cran de la cr√©ation de la r√®gle pour la s√©lectionner. Prenez le r√¥le IAM LabRole pour cette r√®gle. Validez-l√†. Puis allez sur l'√©cran DynamoDB et `Explore items`. Apr√®s un certains temps, vous allez avoir des enregistrements qui vont apparaitre.

### üï∑ü¶∏‚Äç‚ôÄÔ∏èD√©couvrir le sch√©ma de donn√©es avec AWS Glue, les requ√™ter avec Amazon Athena

#### üï∑Aws Glue

L'√©tape suivante de notre pipeline va √™tre de requ√™ter nos donn√©es et essayer de produire une information utile. Actuellement nous avons simplement des fichiers json stock√©s dans un bucket S3, autrement dit, loin d'un format de donn√©es facilement requ√™table. En python il faudrait par exemple t√©l√©charger les fichiers, puis les lire avec un biblioth√®que pour ensuite les traiter. Avec de simples fichier dans un bucket S3, nous ne disposons pas du sch√©ma des donn√©es ni d'outils pour les requ√™ter. Nous pourrions les charger dans un cluster EMR (Elastic Map Reduce) comme dans les TP pr√©c√©dents (si vous avez le temps vous pouvez essayer), o√π utiliser une solution encore une fois serverless via AWS Glue et Amazon Athena. AWS Glue est un service d'int√©gration des donn√©es. Il va mettre en forme les donn√©es pour qu'un autre service puisse les utiliser. Voici son sch√©ma de fonctionnement.

![](https://docs.aws.amazon.com/images/glue/latest/dg/images/PopulateCatalog-overview.png)

Le *Data Store* repr√©sente nos donn√©es. AWS glue permet de configurer un *Crawler* qui va d√©terminer leur sch√©ma. Une fois le sch√©ma √©tabli, AWS Glue va √©crire cela dans votre *Data Catalog* pour permettre aux services Amazon de requ√™ter nos donn√©es.

Cherchez le service AWS Glue et cliquez sur `Crawlers` puis `Create crawler`. Comme habituellement, donnez un nom explicite √† votre crawler, puis sur la page suivante s√©lectionnez un source de donn√©es de type S3, et trouvez votre bucket. Ne crawlez que les nouveaux dossiers avec un √©chantillon de 10 fichiers. Nos fichiers vont tous avoir un sch√©ma similaire alors pour acc√©l√©rer les traitements nous allons en prendre seulement une partie. Sur l'√©cran suivant s√©lectionnez le r√¥le LabRole. Cr√©ez une base de donn√©e pour enregistrer le r√©sultat du crawler, et conserver l'option `On demand` pour le crawler. Valider tout et lancez votre crawler. Il devrez prendre environ 1 min. Vous pouvez maintenant aller sur la page `Tables` et voir qu'une table est apparrue. Si vous cliquez dessus, vous aurez le sch√©ma de votre table.

#### ü¶∏‚Äç‚ôÄÔ∏èAmazon Athena

Il est maintenant temps de requ√™ter nos donn√©es avec Amazon Athena. Cherchez le service Athena. Vous allez arriver une un √©cran de requ√™tage. Dans la partie de gauche, s√©lectionner votre base de donn√©es. Une table devrait apparaitre dans la partie `Tables`. Cliquer sur les 3 points horizontaux (cela s'appelle un kebab en UI) puis preview table. Une requ√™te va apparaitre dans l'√©cran central, ex√©cutez-l√†. Amazon Athena va vous demander de choisir un bucker pour enregistrer les r√©sultats, choisissez le bucket `iot-tutorial-athena-query-...`

> Si vous obtenez une erreur du type `HIVE_PARTITION_SCHEMA_MISMATCH` il vous faut modifier votre crawler et activer l'option `Update all new and existing partitions with metadata from the table` qui se trouve au step 4 dans les options avanc√©es. Relanc√© ensuite votre crawler.

Vous pouvez maintenant requ√™ter vos donn√©es. Essayez de calculer des indicateurs simples comme la temp√©rature moyenne, minimale et maximale par station, la m√™me chose par type de condition climatique. Explorer un peu vos donn√©es pour vous familiariser avec-elles. Pour rappel, bien que vous puissiez avoir l'impression de manipuler une base de donn√©es ce n'est pas le cas. Chaque requ√™te va allez lire vos fichiers json dans votre bucket S3. Mais gr√¢ce au sch√©ma d√©termin√© par AWS Glue et le moteur de requ√™te Amazon Athena nous manipulons nos donn√©es comme si elles √©taient dans une base de donn√©es. Le tout avec des outils 100% serverless o√π l'on paye √† l'utilisation (ce qui a des avantages et des inconv√©nients).

### üìäVisualisation des donn√©es avec Amazon Quicksight

#### üìùMise en forme des donn√©es

Avant d'aller plus loin, comme Amazon Quicksight ne peut pas traiter des donn√©es non tabulaire, nous allons devoir mettre en forme nos donn√©es. Cela va revenir √† cr√©er une nouvelle table dans notre base de donn√©es √† partir de la table que nous avons d√©j√† transformant le sch√©ma. Vous allez faire cela avec une requ√™te de la forme `CREATE TABLE IF NOT EXISTS nom_table AS SELECT ... FROM "database"."table"`. 

Les champs que nous voulons sont :

- Champs directement accessibles :  weather_station, latitude, longitude, elevation, time
- Champs encapsul√©s dans un autre : air_temperature.value, dew_point.value, wind_speed.value, sea_level_pressure.value, sky_ceiling.value, visibility_distance.value
- Champs de type vecteur qu'il va falloir *unnester* : sky_covover_conction.cloud_type, weather_condition.present_weather_condition. Pour ces derniers vous allez devoir faire des `CROSS JOIN UNNEST(field) AS t(renamed_field)` pour arriver √† les *unnester*.

Quand votre table est cr√©√©e, requ√™tez l√† pour voir si tout va bien. Vous pouvez maintenant passez √† la partie finale du TP √† savoir, visualisez vos donn√©es.

#### üìäQuicksight

Amazon Quicksight est un service de *business inteligence* serverless. Il prend la charge la r√©cup√©ration des donn√©es et leurs traitements pour cr√©er des dashboards √† partir de donn√©es diverses. Comme il est serverless il permet de passer √† l'√©chelle facilement. N√©anmoins c'est un service cher qui demande un abonnement pour chaque utilisateur du service. Pour ce TP cela ne va pas poser probl√®me, mais la facture peut exploser rapidement en entreprise. D'autres outils de visualisation existent comme [Tableau](https://www.tableau.com/fr-fr) ou [Kibana](https://www.elastic.co/fr/kibana/), mais seraient plus long √† utiliser dans notre contexte.

Cherchez le service Quicksight. Il vous faut cr√©er un compte sp√©cifique pour Quicksight. S√©lectionnez l'option entreprise, ignorez l'avertissement : "This IAM user or role may not have all the correct permissions...". Pour la m√©thode d'authentification choisissez "Use IAM federated identities & QuickSight-managed users" et pour le role IAM `LabRole`

Une fois connect√© √† Quicksight, cr√©ez une nouvelle analyse avec le bouton en haut √† droite. S√©lectionnez `New datasets` puis `Athena`. Donnez le nom que vous souhaitez √† votre source, puis s√©lectionnez votre base de donn√©es et enfin la table que vous venez de cr√©er. Importez la dans SPICE et validez.

Une fois sur l'√©cran de dashboard vous pouvez cr√©ez les graphiques que vous souhaitez en drag&dropant les variables dans l'AutoGraph. Avec l'option `ADD` vous pouvez ajouter des visualisations. Cr√©ez des visualisations qui vous semblent pertinentes.